# This is where to define jobs and pipelines for
# the relevant flow.
# 
# The structure follows the Jobs API
# and the Delta Live Tables API for jobs and 
# pipelines respectively.
#
# Jobs API: https://docs.databricks.com/dev-tools/api/latest/jobs.html
# Delta Live Tables API: https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html
#
# Below are examples of a job and a pipeline

jobs:
- name: "trips-classification-model-training"
  new_cluster:
    spark_version: "10.5.x-cpu-ml-scala2.12"
    num_workers: 1
    node_type_id: "Standard_DS3_v2"
  notebook_task:
    notebook_path: "projects/nyctaxi/flows/ml/training/classification_model"
  git_source:
    git_branch: "main"

# pipelines:
# - name: "mlopstemplate-data-pipeline"
#   libraries:
#   - notebook:
#       path: "projects/mlopstemplate/flows/prep/data_pipeline"
#   target: "dlt"
#   clusters:
#   - label: "default"
#     autoscale:
#       min_workers: 1
#       max_workers: 5
#   continous: false
