# This is where to define jobs and pipelines for
# the relevant flow.
# 
# The structure follows the Jobs API
# and the Delta Live Tables API for jobs and 
# pipelines respectively.
#
# Jobs API: https://docs.databricks.com/dev-tools/api/latest/jobs.html
# Delta Live Tables API: https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html
#
# Below are examples of a job and a pipeline
#
# jobs:
# - name: "mlopstemplate-training-job"
#   new_cluster:
#     spark_version: "10.3.x-cpu-ml-scala2.12"
#     num_workers: 1
#     node_type_id: "Standard_DS3_v2"
#   notebook_task:
#     notebook_path: "projects/mlopstemplate/flows/ml/training/model_training"

# pipelines:
# - name: "mlopstemplate-data-pipeline"
#   libraries:
#   - notebook:
#       path: "projects/mlopstemplate/flows/prep/data_pipeline"
#   target: "dlt"
#   clusters:
#   - label: "default"
#     autoscale:
#       min_workers: 1
#       max_workers: 5
#   continous: false
